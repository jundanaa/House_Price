import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
import xgboost as XGB
from xgboost import XGBRegressor
import sklearn.metrics as metrics
import math
from scipy.stats import norm, skew
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

train=pd.read_csv('train.csv')
test=pd.read_csv('test.csv')
print ('train data')
display (train.head())
print ('#'*50)
print ('test data')
display (test.head())
print ('#'*50)
display ('train shape',train.shape)
print ('#'*50)
display ('test shape',test.shape)

#### concat data

# EDA



train.info()

test.info()

cat=train.select_dtypes(include='object').columns
num=train.select_dtypes(exclude='object').columns

train.describe().T

train[cat].describe().T



total = train.isnull().sum().sort_values(ascending=False)
percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)

cols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',
               'GarageQual','GarageCond','GarageFinish','GarageType', 'Electrical',
               'KitchenQual', 'SaleType', 'Functional', 'Exterior2nd', 'Exterior1st',
               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2',
               'MSZoning', 'Utilities']

# replace 'NaN' with 'None' in these columns
for col in cols_fillna:
    train[col].fillna('None',inplace=True)
    test[col].fillna('None',inplace=True)

total = train.isnull().sum().sort_values(ascending=False)
percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(5)

ha=['LotFrontage', 'MasVnrArea']
train.fillna(train[ha].mean(), inplace=True)
test.fillna(test[ha].mean(), inplace=True)

total = train.isnull().sum().sort_values(ascending=False)
percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(10)

total = test.isnull().sum().sort_values(ascending=False)
percent = (test.isnull().sum()/test.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(10)

for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageArea', 'GarageCars'):
    test[col] = test[col].fillna(0)


  test['GarageYrBlt'] = test['GarageYrBlt'].fillna(0)
  train['GarageYrBlt'] = train['GarageYrBlt'].fillna(0)

train.isnull().sum().sum()

test.isnull().sum().sum()

skewness_kurtosis_results = {}
for col in train[num].columns:
    skewness = train[col].skew()
    kurtosis = train[col].kurtosis()
    skewness_kurtosis_results[col] = {'Skewness': skewness, 'Kurtosis': kurtosis}

# Mengonversi hasil menjadi DataFrame
results_df = pd.DataFrame.from_dict(skewness_kurtosis_results, orient='index')

# Menampilkan tabel
print(results_df)

kolom yang memiliki skewness dan kurtosis akan di normalkan menggunakan yeo-jhonshon



#### numerical data

corrmat = train[num].corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True);

corr = train[num].corr()
important_num_cols = list(train[num].corr()["SalePrice"][(train[num].corr()["SalePrice"]>0.50) | (train[num].corr()["SalePrice"]<-0.50)].index)
plt.figure(figsize=(10,10))
g = sns.heatmap(train[important_num_cols].corr(),annot=True,cmap="Blues")

corr['SalePrice'].sort_values(ascending=False)

features that have high corelation woth target is
1. OverallQual      0.790982
2. GrLivArea        0.708624
3. GarageCars       0.640409
4. GarageArea       0.623431
5. TotalBsmtSF      0.613581
6. 1stFlrSF         0.605852
7. FullBath         0.560664
8. TotRmsAbvGrd     0.533723
9. YearBuilt        0.522897
10. YearRemodAdd     0.507101

but there are feature that have multicolinearity with others. there are is:
1. TotalBsmtSF and 1stFlrSF
2. GrLivArea and TotRmsAbvGrd
3. GarageCars and GarageArea


so i decide to drop one of them, looking by corelation with target

#### categorical data

for column in cat:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=column, y='SalePrice', data=train)
    plt.title(f'box Plot of {column} by Target')
    plt.show()

Conclusion from EDA on categorical columns:

For many of the categorical there is no strong relation to the target.
However, for some fetaures it is easy to find a strong relation.
From the figures above these are : 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType' Also for the categorical features, I use only those that show a strong relation to SalePrice.


So the other columns are dropped when creating the ML dataframes in Part 2 :
'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterCond', 'Foundation', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleCondition'


source : https://www.kaggle.com/code/dejavu23/house-prices-eda-to-ml-beginner

# data pre-processing

### outliers

fig, ax = plt.subplots()
ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])
plt.ylabel('SalePrice', fontsize=13)
plt.xlabel('GrLivArea', fontsize=13)
plt.show()

train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)

#Check the graphic again
fig, ax = plt.subplots()
ax.scatter(train['GrLivArea'], train['SalePrice'])
plt.ylabel('SalePrice', fontsize=13)
plt.xlabel('GrLivArea', fontsize=13)
plt.show()

id_test =test['Id']





train.head()

train.info()

train['Age']=train['YrSold']-train['YearBuilt']

train.info()

num=train.select_dtypes(exclude='object').columns
corrmat = train[num].corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True);

corr = train[num].corr()
important_num_cols = list(train[num].corr()["SalePrice"][(train[num].corr()["SalePrice"]>0.50) | (train[num].corr()["SalePrice"]<-0.50)].index)
plt.figure(figsize=(10,10))
g = sns.heatmap(train[important_num_cols].corr(),annot=True,cmap="Blues")

num_strong=['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'Age']
catg_strong_corr = [ 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual',
                     'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']

importance_cols=num_strong+catg_strong_corr

importance_cols

# Menentukan kolom-kolom yang ingin digunakan


# Memilih kolom-kolom tersebut dari data train dan test
train = train[importance_cols]
#

# # Menampilkan data train dan test yang telah dipilih kolomnya
# print("Data Train:")
# print(train)
# print("\nData Test:")
# print(test)


train.head()

test['Age']=test['YrSold']-test['YearBuilt']

yaa=['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'Age','MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual',
                     'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']
test=test[yaa]

test.head()

train.shape

test.shape

train.describe()

cat=train.select_dtypes(include='object').columns

train[cat].describe()

plt.figure(figsize=(12, 8))
for i, feature in enumerate(num_strong, 1):
    plt.subplot(3, 3, i)
    sns.distplot(train[feature], kde=True)
    plt.title(f"{feature} Skewness: {train[feature].skew():.2f}")
    plt.xlabel(feature)
    plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

ccy=['SalePrice','GrLivArea','TotalBsmtSF']


for feature in ccy:
    train[feature + '_log'] = np.log1p(train[feature])

# Hapus kolom yang sudah tidak digunakan
train = train.drop(ccy, axis=1)


train.info()

jhon=['GrLivArea','TotalBsmtSF']
import numpy as np

for feature in jhon:
    test[feature + '_log'] = np.log1p(test[feature])

# Hapus kolom yang sudah tidak digunakan
test = test.drop(jhon, axis=1)


#train.drop(['SalePrice', 'GrLivArea', 'TotalBsmtSF'], axis=1, inplace=True)

#test.drop([ 'GrLivArea', 'TotalBsmtSF'], axis=1, inplace=True)

num=train.select_dtypes(exclude='object').columns

plt.figure(figsize=(12, 8))
for i, feature in enumerate(num, 1):
    plt.subplot(3, 3, i)
    sns.histplot(train[feature], kde=True)
    plt.title(f"{feature} Histogram")
    plt.xlabel(feature)
    plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

train.head()

test.head()

### cat -> num

convert feature categorical menjadi numerical


thanks to : https://www.kaggle.com/code/dejavu23/house-prices-eda-to-ml-beginner
for such amazing idea





for catg in cat :
    #sns.catplot(x=catg, y=target, data=df_train, kind='boxen')
    sns.violinplot(x=catg, y='SalePrice_log', data=train)
    plt.show()

for catg in cat :
    g = train.groupby(catg)['SalePrice_log'].mean()
    print(g)

msz_catg2 = ['RM', 'RH']
msz_catg3 = ['RL', 'FV']


# Neighborhood
nbhd_catg2 = ['Blmngtn', 'ClearCr', 'CollgCr', 'Crawfor', 'Gilbert', 'NWAmes', 'Somerst', 'Timber', 'Veenker']
nbhd_catg3 = ['NoRidge', 'NridgHt', 'StoneBr']

# Condition2
cond2_catg2 = ['Norm', 'RRAe']
cond2_catg3 = ['PosA', 'PosN']

# SaleType
SlTy_catg1 = ['Oth']
SlTy_catg3 = ['CWD']
SlTy_catg4 = ['New', 'Con']

for df in [train, test]:

    df['MSZ_num'] = 1
    df.loc[(df['MSZoning'].isin(msz_catg2) ), 'MSZ_num'] = 2
    df.loc[(df['MSZoning'].isin(msz_catg3) ), 'MSZ_num'] = 3

    df['NbHd_num'] = 1
    df.loc[(df['Neighborhood'].isin(nbhd_catg2) ), 'NbHd_num'] = 2
    df.loc[(df['Neighborhood'].isin(nbhd_catg3) ), 'NbHd_num'] = 3

    df['Cond2_num'] = 1
    df.loc[(df['Condition2'].isin(cond2_catg2) ), 'Cond2_num'] = 2
    df.loc[(df['Condition2'].isin(cond2_catg3) ), 'Cond2_num'] = 3

    df['Mas_num'] = 1
    df.loc[(df['MasVnrType'] == 'Stone' ), 'Mas_num'] = 2

    df['ExtQ_num'] = 1
    df.loc[(df['ExterQual'] == 'TA' ), 'ExtQ_num'] = 2
    df.loc[(df['ExterQual'] == 'Gd' ), 'ExtQ_num'] = 3
    df.loc[(df['ExterQual'] == 'Ex' ), 'ExtQ_num'] = 4

    df['BsQ_num'] = 1
    df.loc[(df['BsmtQual'] == 'Gd' ), 'BsQ_num'] = 2
    df.loc[(df['BsmtQual'] == 'Ex' ), 'BsQ_num'] = 3

    df['CA_num'] = 0
    df.loc[(df['CentralAir'] == 'Y' ), 'CA_num'] = 1

    df['Elc_num'] = 1
    df.loc[(df['Electrical'] == 'SBrkr' ), 'Elc_num'] = 2


    df['KiQ_num'] = 1
    df.loc[(df['KitchenQual'] == 'TA' ), 'KiQ_num'] = 2
    df.loc[(df['KitchenQual'] == 'Gd' ), 'KiQ_num'] = 3
    df.loc[(df['KitchenQual'] == 'Ex' ), 'KiQ_num'] = 4

    df['SlTy_num'] = 2
    df.loc[(df['SaleType'].isin(SlTy_catg1) ), 'SlTy_num'] = 1
    df.loc[(df['SaleType'].isin(SlTy_catg3) ), 'SlTy_num'] = 3
    df.loc[(df['SaleType'].isin(SlTy_catg4) ), 'SlTy_num'] = 4

target='SalePrice_log'

from scipy import stats

new_col_num = ['MSZ_num', 'NbHd_num', 'Cond2_num', 'Mas_num', 'ExtQ_num', 'BsQ_num', 'CA_num', 'Elc_num', 'KiQ_num', 'SlTy_num']

nr_rows = 4
nr_cols = 3
fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(16, 12))

for r in range(nr_rows):
    for c in range(nr_cols):
        i = r * nr_cols + c
        if i < len(new_col_num):
            sns.regplot(x=train[new_col_num[i]], y=train[target], ax=axs[r][c])
            stp = stats.pearsonr(train[new_col_num[i]], train[target])
            str_title = f"r = {stp[0]:.2f}      p = {stp[1]:.2f}"
            axs[r][c].set_title(str_title)

plt.tight_layout()
plt.show()

catg_cols_to_drop = ['Neighborhood' , 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']


for df in [train, test] :
    df.drop(catg_cols_to_drop, inplace= True, axis = 1)

train.head()

test.drop(['MSZoning'], axis=1, inplace=True)

train.drop(['MSZoning'], axis=1, inplace=True)

train.head()

test.head()

train['OverallQual'].unique()

train_sc=train.copy()
test_sc=test.copy()

train_sc.head()

from sklearn.preprocessing import StandardScaler
features_to_scale = ['OverallQual', 'GarageCars', 'FullBath', 'Age', 'MSZ_num', 'NbHd_num', 'Cond2_num', 'Mas_num', 'ExtQ_num', 'BsQ_num', 'CA_num', 'Elc_num', 'KiQ_num', 'SlTy_num']

scaler = StandardScaler()
train_sc[features_to_scale] = scaler.fit_transform(train_sc[features_to_scale])

# Split data into features and target
X = train_sc.drop(columns=['SalePrice_log'])
y = train_sc['SalePrice_log']

X.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


y_test.head()

# Modeling

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error

# Inisialisasi model-model yang akan digunakan
models = {
    "Linear Regression": LinearRegression(),
    "Lasso": Lasso(alpha=0.1, random_state=42),
    "Elastic Net": ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "Support Vector Regression": SVR()
}

# Melatih model dan menyimpan hasilnya
results = []

for name, model in models.items():
    # Melatih model
    model.fit(X_train, y_train)
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # Menghitung metrik evaluasi untuk data latih dan uji
    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))
    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))
    mape_train = mean_absolute_percentage_error(y_train, y_train_pred)
    mape_test = mean_absolute_percentage_error(y_test, y_test_pred)
    r2_train = r2_score(y_train, y_train_pred)
    r2_test = r2_score(y_test, y_test_pred)

    # Cross-validation R²
    r2_cv_train = np.mean(cross_val_score(model, X_train, y_train, cv=5, scoring='r2'))
    r2_cv_test = np.mean(cross_val_score(model, X_test, y_test, cv=5, scoring='r2'))

    results.append({
        "Model": name,
        "RMSE (train)": rmse_train,
        "RMSE (test)": rmse_test,
        "MAPE (train)": mape_train,
        "MAPE (test)": mape_test,
        "R² (train)": r2_train,
        "R² (test)": r2_test,
        "R² (cross-val train)": r2_cv_train,
        "R² (cross-val test)": r2_cv_test
    })


# Mengubah hasil menjadi DataFrame
results_df = pd.DataFrame(results)

# Menampilkan hasil
display(results_df)

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

model = RandomForestRegressor()

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}


grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')

grid_search.fit(X_train, y_train)


print("Best Parameters: ", grid_search.best_params_)
print("Best Score: ", grid_search.best_score_)


from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score

# Model dengan parameter terbaik
best_model = RandomForestRegressor(max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50)
best_model.fit(X_train, y_train)

# Prediksi pada test set
y_pred_train = best_model.predict(X_train)
y_pred_test = best_model.predict(X_test)

# Evaluasi performa pada data latih
rmse_train = mean_squared_error(y_train, y_pred_train, squared=False)
mape_train = mean_absolute_percentage_error(y_train, y_pred_train)
r2_train = r2_score(y_train, y_pred_train)

# Evaluasi performa pada data uji
rmse_test = mean_squared_error(y_test, y_pred_test, squared=False)
mape_test = mean_absolute_percentage_error(y_test, y_pred_test)
r2_test = r2_score(y_test, y_pred_test)

# Cross-validation
cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='r2')

# Output results
print(f"RMSE (Train): {rmse_train:.4f}")
print(f"MAPE (Train): {mape_train:.4f}")
print(f"R² (Train): {r2_train:.4f}")
print(f"RMSE (Test): {rmse_test:.4f}")
print(f"MAPE (Test): {mape_test:.4f}")
print(f"R² (Test): {r2_test:.4f}")
print(f"Cross-Validation R²: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}")


y_test_pred = best_model.predict(X_test)

X_test.head()

y_test_pred



test_sc.head()

test_prediction=best_model.predict(test_sc)


test_prediction

predicted_prices = np.exp(test_prediction)

test_prediction_or=pd.DataFrame(predicted_prices,columns=['SalePrice'])

test_prediction_or.head()

result=pd.concat([id_test,test_prediction_or],axis=1)

result.head()

we will update soon!



## reff
1. https://www.kaggle.com/code/serigne/stacked-regressions-top-4-on-leaderboard
2. https://www.kaggle.com/code/adamml/how-to-be-in-top-10-for-beginner
3. https://www.kaggle.com/code/emrearslan123/house-price-prediction
4. https://www.kaggle.com/code/dejavu23/house-prices-eda-to-ml-beginner

